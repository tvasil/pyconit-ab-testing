{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fe569b",
   "metadata": {},
   "source": [
    "# Monitoring and Analysis of the experiment\n",
    "\n",
    "So, your experiment is finally live! What should you do before resting on your laurels? Well of course, check that all is going well. What does this typically mean?\n",
    "\n",
    "- Construct a monitoring dashboard. This is ONLY meant to catch technical issues. In the frequentist approach to A/B testing (what we've been doing so far), you should not be checking on a daily basis how your metrics are performing, because you might be a victim \"peeking\". \n",
    "- Check that your experiment split is actually what it theoretically should be. I.e. conduct a hypothesis test that, if the split between test/control is 50/50, then the actual proportion of users bucketed into the test group is no different than 0.5. \n",
    "\n",
    "And if you manage to do all of this...might as well reuse it for analysis later! Let's build all of this as a dashboard using Panel. \n",
    "\n",
    "## 1. Monitoring sample ratio mismatch (SRM)\n",
    "\n",
    "Sample ratio mismatch is the situation where the ratio of samples being bucketed into one of the experimental groups is **significantly different** to what would be expected in theory. For example: \n",
    "- You expect that 50% of new users will be bucketed in to the test group. \n",
    "- Instead, out of a total of 20,000 users, 18,000 have been bucketed into the test group. \n",
    "\n",
    "Why do we care?\n",
    "\n",
    "- Sample ratio mismatch is often one of the most important signals that something is wrong with the setup of the experiment. \n",
    "- Examples: app users who have bad internet connections get bucketed to \"control\". \n",
    "- It's better to stop the experiment early and fix the bug, than potentially sacrifice millions of observations. Remember, once a user has seen something they cannot unsee it!\n",
    "\n",
    "If you are at the end of an experiment, you can ran a simple one-sample z-test to determine: \n",
    "\n",
    "H_0: Probability of being bucketed into test group = 0.5\n",
    "H_1: Probability of being bucketed into test group <> 0.5\n",
    "\n",
    "However, since we are monitoring, we will not have the experiment completed. Every day we are getting new samples. In the first few days, as the experiment is ramping up, the numbers might be very low. By running a t-test every day, we can become victims of **peeking** and an increased false positive rate. This is true for both **metrics** and **sample ratio**. See below: \n",
    "\n",
    "![peeking](../img/peeking.png)\n",
    "\n",
    "The solution: If your experiment is still running, you need to use a Sequential Testing Approach, or some other methodology to avoid inflating your false positives. We will use the small package `ssrm-test`, developed by Michael Lindon at Optimizely, which helps us discover whether the experiment has sample ratio mismatch, in a control (sequential) manner. You can check out the full demo notabook for `ssrm` [here](https://github.com/ajmasci/ssrm/blob/master/notebooks/introduction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssrm_test import ssrm_test\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False, 'figure.figsize':(14, 8)}\n",
    "sns.set_theme(style=\"whitegrid\", rc=custom_params, palette='bright')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_assignments = pd.read_csv(\"../data/first_experiment_assignments.csv\", parse_dates=['bucketed_at'])\n",
    "first_assignments.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_pct = #TODO\n",
    "test_pct = #TODO\n",
    "print(f\"Empirical sample ratios are {control_pct:.2%} and {test_pct:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of unique users per day in each bucket / test_group\n",
    "counts_by_day = #TODO\n",
    "sns.lineplot(data=counts_by_day)\n",
    "plt.title(\"Split of bucketed traffic over time\", loc='left', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ec3c5",
   "metadata": {},
   "source": [
    "There probably is a problem! Let's run the data through `ssrm` to make sure we are not tricked by probability. \n",
    "\n",
    "\n",
    "_______\n",
    "\n",
    "**SSRM**\n",
    "\n",
    "First, we need to get our sample into a 2 dimensional numpy array format. Each element should be [control_val, test_val], i.e. a dummified representation of the group that the observation was in. We can easily do this with `pd.get_dummies()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(#TODO).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the probabilities / ratios that we assume should be the Null hypothesis (usually 50/50%) as a list\n",
    "p_0 = #TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9917da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = ssrm_test.sequential_p_values(#TODO, #TODO)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.plot(pvals)\n",
    "fig.suptitle(\"Sequential p-value for Testing SRM\", fontsize=20)\n",
    "plt.xlabel(\"Visitor Count\", fontsize=16)\n",
    "plt.ylabel(\"p-value\", fontsize=16)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d751dce",
   "metadata": {},
   "source": [
    "**What conclusion would you come to from this chart and p-value?**\n",
    "\n",
    "![confounding](../img/confounding_variable.png)\n",
    "\n",
    "_Credit: [Lauren Thomas](https://www.scribbr.co.uk/research-methods/confounding-variable/)_\n",
    "\n",
    "- The reasonable conclusion is that **something is wrong with the test setup that is causing a bias in the bucketing / randomization of users**. \n",
    "- These biases (even small ones) can fundamentally undermine the validity of the analysis and render the results unusable. Imagine, for example, if all users who have a bad internet connection get bucketed into control. Perhaps those users are also in locations where we don't ship our products to, so even if they browse, they **never** end up converting (extreme example). This is likely to bring down the average of the control group, artificially inflating the difference in means in favor of the test group. \n",
    "- There are two possible approaches: \n",
    "    - If you can fully identify the users that are receiving biased treatment and if you can guarantee that it's **only** them, then you can continue running the experiment, but remove those users from the later analysis. \n",
    "    - The best approach, however, is to stop the experiment, fix the issue and restart the experiment from a clean slate (with new randomization). **Note**: Users who were bucketed in the first iteration should be excluded from the subsequent analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd1270",
   "metadata": {},
   "source": [
    "## 2. Monitoring metrics\n",
    "\n",
    "For each metric that we want to measure, we need to: \n",
    "- measure the values between the test groups\n",
    "- keep track of how the different groups are performing over time\n",
    "- visualize them in a compelling way, including any necessary splits, without compromising statistical interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16950eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = pd.read_csv(\"../data/post_sessions.csv\", parse_dates=['visit_started_at'])\n",
    "payment_page_visits = pd.read_csv(\"../data/post_payment_page_visits.csv\", parse_dates=['payment_page_accessed_at'])\n",
    "purchases = pd.read_csv(\"../data/post_purchases.csv\", parse_dates=['purchase_processed_at'])\n",
    "assignments = pd.read_csv(\"../data/experiment_assignments.csv\", parse_dates=['bucketed_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the dataframes together\n",
    "post_df = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a column for conversion rate to purchase for those that accessed the payment page\n",
    "post_df['converted'] = #TODO\n",
    "# Calculate a column for conversation rate to successful purchase for those that accessed the payment page\n",
    "post_df['converted_success'] = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count again unique users per day\n",
    "counts_by_day = #TODO\n",
    "sns.lineplot(data=counts_by_day)\n",
    "plt.title(\"Split of bucketed traffic over time\", loc='left', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05a2ee",
   "metadata": {},
   "source": [
    "**a) For the primary decision metric and each experimental group, compute observations, mean value, standard deviation, relative difference, 95% confidence interval of difference**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1274c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is our metric performing at a first glance?\n",
    "post_df[~post_df.payment_page_accessed_at.isna()].groupby('test_group')[['converted_success', 'user_id']].agg(\n",
    "    {'converted_success': ['mean', 'std'],\n",
    "     'user_id': ['nunique', 'count']}\n",
    ")\n",
    "\n",
    "# metric, mean_control, mean_test, N observations, diff absolute, diff relative, diff_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_group(df: pd.DataFrame, \n",
    "                       metric_col:str, \n",
    "                       control_name:str='control', \n",
    "                       test_name:str='test') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a dataframe with the summary statistics of a specific metric, grouped by test_group\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count the total observations and successes (successful purchases) in each group\n",
    "    counts_control = #TODO\n",
    "    nobs_control = #TODO\n",
    "\n",
    "    counts_test = #TODO\n",
    "    nobs_test = #TODO\n",
    "    \n",
    "    # Calculate the mean success rate per group, the absolute and relative difference.\n",
    "    mean_control = #TODO\n",
    "    mean_test = #TODO\n",
    "    \n",
    "    diff_abs = #TODO\n",
    "    diff_rel = #TODO\n",
    "    \n",
    "    # Calculate the pooled standard error to get 95% confidence intervals on the difference between the means\n",
    "    # See more about this here: \n",
    "    # http://ethen8181.github.io/machine-learning/ab_tests/frequentist_ab_test.html#Comparing-Two-Proportions\n",
    "    pooled_var = #TODO\n",
    "    pooled_se = np.sqrt(pooled_var)c\n",
    "    pooled_se\n",
    "    \n",
    "    ci_lower, ci_upper = diff_abs - 1.96 * pooled_se, diff_abs + 1.96 * pooled_se\n",
    "    \n",
    "    return pd.Series({'metric': metric_col,\n",
    "                      'counts_'+control_name: nobs_control,\n",
    "                      'counts_'+test_name: nobs_test,\n",
    "                      'mean_'+control_name: mean_control,\n",
    "                      'mean_'+test_name: mean_test,\n",
    "                      'diff_abs': diff_abs,\n",
    "                      'diff_rel': diff_rel,\n",
    "                      'ci_95%': f'[{ci_lower:.3f} to {ci_upper:.3f}]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try the function on the Payment page > successful conversion metric\n",
    "summarize_by_group(#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2cc43",
   "metadata": {},
   "source": [
    "**b) Split the results by new/repeat user and platform used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7424b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same function we wrote previously\n",
    "def clean_platform(platform:str) -> str:\n",
    "    \"\"\"Returns whether a user is on Android, iOS or any kind of Desktop device\"\"\"\n",
    "    if 'Android' in platform:\n",
    "        return 'android'\n",
    "    if ('iPad' in platform) or ('iPhone' in platform):\n",
    "        return 'ios'\n",
    "    else:\n",
    "        return 'desktop-other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e48cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df['new_repeat'] = post_df['last_purchase'].apply(lambda x: 'new' if pd.isna(x) else 'repeat')\n",
    "post_df['platform_clean'] = post_df.platform.apply(lambda x: clean_platform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c616ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df_new_users = #TODO\n",
    "summarize_by_group(#TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ef42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df_repeat_users = #TODO\n",
    "summarize_by_group(#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6eca7",
   "metadata": {},
   "source": [
    "**Repeat a) and b) for all secondary metrics**\n",
    "\n",
    "Now you see, it would benefit to write a function to show us a variety of splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6228aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(base_df:pd.DataFrame, cols_to_split_on:list, metrics:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a set of columns to split on and a set of metrics to calculate,\n",
    "    this function returns a Pandas dataframe with one row for every metric and user group\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for m in metrics:\n",
    "        summary = #TODO\n",
    "        summary['user_group'] = 'all-up'\n",
    "        result.append(#TODO)\n",
    "        \n",
    "        for col in #TODO:\n",
    "            unique_vals = #TODO\n",
    "            for val in unique_vals:\n",
    "                df = #TODO\n",
    "                summary = #TODO\n",
    "                summary['user_group'] = #TODO\n",
    "                result.append(#TODO)\n",
    "                \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the function!\n",
    "calculate_metrics(base_df=post_df[~post_df.payment_page_accessed_at.isna()],\n",
    "                  cols_to_split_on=['new_repeat', 'platform_clean'],\n",
    "                  metrics=['converted', 'converted_success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b03a65",
   "metadata": {},
   "source": [
    "**Write a function that will plot the metric evolution over time for any given metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1089e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_over_time(df:pd.DataFrame, metric_col:str, date_col:str, aggregation:str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    For a given metric, plots the evolution of this metric over time. \n",
    "    \"\"\"\n",
    "    per_day = #TODO\n",
    "    \n",
    "    fig = plt.Figure(figsize=(10, 6))\n",
    "    ax  = fig.add_subplot()\n",
    "    sns.lineplot(data=#TODO, x=#TODO, y=#TODO, hue=#TODO, ax=#TODO)\n",
    "    ax.set_title(f\"Evolution of {metric_col.upper()} over time: {aggregation}\", loc='left', size=16)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_over_time(df=post_df, metric_col='converted', date_col='bucketed_at', aggregation='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d2a3a7",
   "metadata": {},
   "source": [
    "**What about statistical testing and p-values? Why didn't we include those?**\n",
    "Showing p-values during the experiment (before we've reached the desired sample size) can mean not only that our results will not be significant for a while, but also checking every day WILL lead to an increased probability of seeing at least 1 false positive (calling the experiment a \"success\" when it's actually not). \n",
    "\n",
    "Also, when we add more metrics and more slices of the same data, we might be actually doing what is called p-hacking:\n",
    "\n",
    "> **p-hacking**: is the misuse of data analysis to find patterns in data that can be presented as statistically significant, thus dramatically increasing and understating the risk of false positives. This is done by performing many statistical tests on the data and only reporting those that come back with significant results. [Wikipedia](https://en.wikipedia.org/wiki/Data_dredging)\n",
    "\n",
    "\n",
    "Some guidelines on p-values and calling anything significant:\n",
    "\n",
    "- Only display \"significance\" values when the agreed on sample size is reached, that is _at the end of the experiment_\n",
    "- For any other metrics or splits, consider adjusting down the p-value that you are comparing to. For example, you could use the [Bonferroni correction](https://mathworld.wolfram.com/BonferroniCorrection.html#:~:text=The%20Bonferroni%20correction%20is%20a,the%20set%20of%20all%20comparisons)) (very strict), or the [Benjamini-Hochberg correction](https://www.biostathandbook.com/multiplecomparisons.html) (a bit harder to calculate).\n",
    "\n",
    "\n",
    "\n",
    "**Let's modify the original function to show us also p-values.** \n",
    "\n",
    "Add some logic to check whether the metric is the primary decision metric or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3cc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_group2(df: pd.DataFrame, \n",
    "                        metric_col:dict, \n",
    "                        primary_decision_metric:bool,\n",
    "                        control_name:str='control', \n",
    "                        test_name:str='test',\n",
    "                        all_up: bool=False,\n",
    "                        alpha:float=0.05) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a dataframe with the summary statistics of a specific metric, grouped by test_group\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count the total observations and successes (successful purchases) in each group\n",
    "    counts_control = df[df.test_group==control_name][metric_col].sum()\n",
    "    nobs_control = df[df.test_group==control_name][metric_col].count()\n",
    "\n",
    "    counts_test = df[df.test_group==test_name][metric_col].sum()\n",
    "    nobs_test = df[df.test_group==test_name][metric_col].count()\n",
    "    \n",
    "    # Calculate the mean success rate per group, the absolute and relative difference.\n",
    "    mean_control = counts_control / nobs_control\n",
    "    mean_test = counts_test / nobs_test\n",
    "    \n",
    "    diff_abs = mean_test - mean_control\n",
    "    diff_rel = (mean_test - mean_control)/mean_control\n",
    "    \n",
    "    # Calculate the pooled standard error to get 95% confidence intervals on the difference between the means\n",
    "    # See more about this here: \n",
    "    # http://ethen8181.github.io/machine-learning/ab_tests/frequentist_ab_test.html#Comparing-Two-Proportions\n",
    "    pooled_var = mean_control * (1 - mean_control) / nobs_control + mean_test * (1 - mean_test) / nobs_test\n",
    "    pooled_se = np.sqrt(pooled_var)\n",
    "    pooled_se\n",
    "    \n",
    "    ci_lower, ci_upper = diff_abs - 1.96 * pooled_se, diff_abs + 1.96 * pooled_se\n",
    "    \n",
    "    if primary_decision_metric and all_up:\n",
    "        metric_type = #TODO\n",
    "        _, p_val = #TODO\n",
    "\n",
    "    else:\n",
    "        metric_type = #TODO\n",
    "        p_val = #TODO\n",
    "        \n",
    "    if p_val < alpha:\n",
    "        sig = '*'\n",
    "    else: \n",
    "        sig = ''\n",
    "    \n",
    "    return pd.Series({'metric': metric_col,\n",
    "                      'metric_type': metric_type,\n",
    "                      'counts_'+control_name: nobs_control,\n",
    "                      'counts_'+test_name: nobs_test,\n",
    "                      'mean_'+control_name: mean_control,\n",
    "                      'mean_'+test_name: mean_test,\n",
    "                      'diff_abs': diff_abs,\n",
    "                      'diff_rel': diff_rel,\n",
    "                      'ci_95%': f'[{ci_lower:.3f} to {ci_upper:.3f}]',\n",
    "                      'p-value': p_val,\n",
    "                      'sig':sig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics2(base_df:pd.DataFrame, cols_to_split_on:list, metrics:dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a set of columns to split on and a set of metrics to calculate,\n",
    "    this function returns a Pandas dataframe with one row for every metric and user group\n",
    "    \n",
    "    Metrics need to be passed as a dictionary indicating if each metric is the primary\n",
    "    decision metric or not. \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for m, primary in metrics.items():\n",
    "        summary = summarize_by_group2(#TODO)\n",
    "        summary['user_group'] = 'all-up'\n",
    "        result.append(summary)\n",
    "        \n",
    "        for col in cols_to_split_on:\n",
    "            unique_vals = base_df[col].unique()\n",
    "            for val in unique_vals:\n",
    "                df = base_df[base_df[col]==val]\n",
    "                summary = summarize_by_group2(#TODO)\n",
    "                summary['user_group'] = val\n",
    "                result.append(summary)\n",
    "                \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c308f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try it again!\n",
    "calculate_metrics2(base_df=post_df[~post_df.payment_page_accessed_at.isna()],\n",
    "                   cols_to_split_on=['new_repeat', 'platform_clean'],\n",
    "                   metrics={'converted': False, 'converted_success': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d0aeb",
   "metadata": {},
   "source": [
    "_________\n",
    "\n",
    "Let's put this all together into a simple dynamic dashboard using `panel`. \n",
    "\n",
    "`panel` is a library that allows you to build interactive dashboards based off of Jupyter notebooks. In this workshop, we have assumed you are reading your data from a static csv, but you could consider the following setup: \n",
    "\n",
    "- Data updates in data warehouse continuously\n",
    "- You write a SQL query and fetch the data with a Python client in your notebook. \n",
    "- Manipulate the data using Pandas + Matplotlib to get summary results\n",
    "- Wrap everything into a `panel` dashboard\n",
    "- Deploy the dashboard to some server (and perhaps think of how often you want to refresh the data from the database). \n",
    "\n",
    "**Write a function that for each metric will show a table of its most common splits**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16289dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "import param\n",
    "\n",
    "class MetricEvolution(param.Parameterized):\n",
    "    metric_col = param.Selector(objects=['converted', 'converted_success'])\n",
    "    aggregation = param.Selector(objects=['mean', 'count'])\n",
    "\n",
    "    def view(self):\n",
    "        return plot_metric_over_time(\n",
    "            df=post_df[~post_df.payment_page_accessed_at.isna()], \n",
    "            metric_col=self.metric_col, \n",
    "            date_col='bucketed_at', \n",
    "            aggregation=self.aggregation)\n",
    "    \n",
    "obj = MetricEvolution()\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d50883",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(obj.param, obj.view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ef0c0",
   "metadata": {},
   "source": [
    "**Write a function that for a given metric, will show its evolution over time in the experiment groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics2(base_df=post_df[~post_df.payment_page_accessed_at.isna()],\n",
    "                   cols_to_split_on=['new_repeat', 'platform_clean'],\n",
    "                   metrics={'converted': False, 'converted_success': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb17d10",
   "metadata": {},
   "source": [
    "## 3. Reporting on experiments \n",
    "\n",
    "Let's try writing down how we would report on this test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ded35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b487accf",
   "metadata": {},
   "source": [
    "**Use function created above to generate results table and visualize them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae4aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c721709",
   "metadata": {},
   "source": [
    "**Summarize your conclusion on the test. Was the null hypothesis rejected? What would you recommend to the product manager?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff61874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyconit",
   "language": "python",
   "name": "pyconit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
